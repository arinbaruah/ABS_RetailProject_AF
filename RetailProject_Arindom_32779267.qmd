---
title: "Retail forecasting project"
author: "Arindom Baruah"
format: html
execute: 
  echo: false
  message: false
  warning: false
number-sections: true
---

```{r}
library(fpp3)
library(tidyverse)
library(kableExtra)
library(latex2exp)
library(plotly)

```

```{r}
#| eval: false

library(fpp3)
get_my_data <- function(student_id) {
  set.seed(student_id)
  all_data <- readr::read_rds("https://bit.ly/monashretaildata")
  while(TRUE) {
    retail <- filter(all_data, `Series ID` == sample(`Series ID`, 1))
    if(!any(is.na(fill_gaps(retail)$Turnover))) return(retail)
  }
}
# Replace the argument with your student ID
retail <- get_my_data(32779267)
```


```{r}
get_my_data <- function(student_id) {
  set.seed(student_id)
  all_data <- fill_gaps(readr::read_rds("https://bit.ly/monashretaildata"))
  while(TRUE) {
    retail <- filter(all_data, `Series ID` == sample(`Series ID`, 1)) 
    if(!any(is.na(retail$Turnover))) return(retail)
  }
}
# Replace the argument with your student ID
retail <- get_my_data(32779267)

```


# Statistical features of the data


## Complete timeseries data {#sec-overall}

Let us observe the time series data for the turnover generated.

```{r}
#| label: fig-turnover
#| fig-cap: "Footwear and personal accessory retailing turnover \n in Western Australia over time"
retail %>% autoplot(Turnover) + labs(x = "Timeline (in Months)",y = "Turnover (mil $)",title="Footwear and personal accessory retailing turnover \n in Western Australia over time",caption = "Source: Australian Bureau of Statistics") + geom_point(data = NULL, aes(x = "2020-02-15", y = 50), color = "red", shape = 1, size = 7) + theme_minimal()


```
:::{.callout-note}
# Key takeaway

Based on @fig-turnover we can observe the following details in the timeseries plot:

1. There appears to be a seasonality in the plot with a peak being observed on every December of a year. __The seasonality additionally appears to be consistent throughout the plot.__

2. The amplitude of the peaks observed in the data appear to be increasing with time. __This indicates that the variation in seasonality of the data is multiplicative in nature.__

3. The plot also indicates the presence of an __additive trend__ in the data with overall turnover increasing every year except for the year of 2020 in an __approximate linear fashion.__ 

4. There appears to be an __outlier in the data during the year of 2020__ as indicated by the <span style=color:red>red cricle</span>. While we expect a drop in retailing turnover in the early months of a year due to a strong retailing period in the previous December, however, __in the year of 2020, the drop in turnover was considerably lower than all the other years.__ This is expected to be caused due to __shutting down of businesses and their activities in the initial part of the COVID-19 pandemic.__
:::

## Variations of trade within a year {#sec-season}



```{r}
#| label: fig-seasonplot
#| fig-cap: "Variation of footwear and personal accessory retailing turnover \n within a year in Western Australia"


ggplotly(retail %>% gg_season(Turnover) + labs(x = "Month",y = "Turnover (mil $)",title="Variation of footwear and personal accessory retailing turnover \n within a year in Western Australia",caption = "Source: Australian Bureau of Statistics")  + theme_minimal(),tooltip = c("Turnover","Month"))

```
:::{.callout-note} 
# Key takeaway
@fig-seasonplot illustrates the variation of turnover within each year generated by the retailers in Western Australia each year. Some key observations are as follows:

1. The turnover generated in Western Australia has been on a rise each year. This suggests that the expenditure of people on footwear and personal accessory retailing services in WA have been increasing each year and could be attributed to the growing population in the state. 

2. We observe that the retailing turnovers in each year are __generally higher in the months of November and December__. This could be a result of the festive period when there are many customers purchasing gifts for families and friends or due to the __special sales promotions such as Boxing Day sales.__

3. Additionally, the month of November also includes the __Black Friday sale__ which attracts multiple customers to purchase items __due to specialised and limited time promotions.__ This may explain the slight rise in turnover during the month of November each year.

4. The __rise in the retail turnovers for the month of December is especially higher during the more recent years (2011-2021)__. This could again be an indicator for the growing population in the country during this period, which has gone on to to create a bigger market and drive sales, hence, contributing to the higher turnover.

5. There appears to be __one single year when the retail turnover dropped steeply in the month of April but eventually picked up for the rest of the year.__ This could be an indicator of the effects of the COVID-19 lockdown when the business activities were very limited and prevented multiple retailers in various regions of Australia, including the state of Western Australia to be able to generate revenue.

:::

## Comparison of turnovers in each month of retail trade

```{r}
#| label: fig-subseries
#| fig-cap: "Variation of footwear and personal accessory retailing turnover in each month of a year in Australia"

retail %>% gg_subseries(Turnover) + labs(x = "Year",y = "Turnover (mil $)",title="Variation of footwear and personal accessory retailing turnover \n in each month of a year in Western Australia",caption = "Source: Australian Bureau of Statistics")  
```

:::{.callout-note}
# Key takeaway

@fig-subseries illustrates the variation of retailing turnover faceted by each month and plotted against years. Key observations are as follows:

1. As already observed in @sec-season, the plot suggests that the __retailing turnover has increased each year from 1990 to 2022.__

2. The average __retail turnover is observed to be higher for the month of December when compared to the rest of the year.__ As explained previously, the main driver for the boost of retailing turnover in this period is attributed to the festive season which includes __Christmas, Boxing Day and the run up to the New Year Eve.__

3. A __drop in retail turnover for the months of March, April and May was observed in 2020.__ This can be attributed to the lockdowns set in place when the pandemic outbreak was first experienced.
:::

# Transformation and differencing of the data {#sec-transform}


In @sec-overall, we have determined that the timeseries data in hand has multiplicative variation in seasonality and a linear trend. This inadvertantly means that we are dealing with non-stationary data.

Converting a non-stationary data to a stationary data is done through the following steps:

## Box-Cox Transformation

In order to apply the Box-Cox transformation, we are required to obtain the appropriate value of the parameter $\lambda$ which will allow us to analyse the timeseries data with the seasonal variations being equal at all levels, thereby transforming the multiplicative variation in the data into an additive variation.

### Estimating the value of $\lambda$ manually {#sec-manual}

In order to apply the Box-Cox transformation, we are required to obtain the appropriate value of the parameter $\lambda$ which will allow us to analyse the timeseries data with the seasonal variations being equal at all levels, thereby transforming the multiplicative variation in the data into an additive variation.

```{r}
#| label: fig-sqrt
#| fig-cap: "Transformation of the annual retailing turnover by square root transformation"
lambda = 0.5 # Square root transformation

retail |>
  autoplot(box_cox(Turnover, lambda)) +
  labs(y = "Square root of Turnover (mil $)",x = "Timeline (in Months)",
       title = latex2exp::TeX(paste0(
         "Transformed annual retailing turnover with $\\lambda$ = ",
         round(lambda,2))),
       caption = "Source: Australian Bureau of Statistics")  + theme_minimal()
```
@fig-sqrt illustrates a square root transformation. As we can observe, the __square root transformation appears to be weak in nature__, and hence, not be able to effectively make the variations in seasonality equal.


Let us now attempt to transform the data using $\lambda = 0.25$

```{r}
#| label: fig-qtr
#| fig-cap: "Transformation of the annual retailing turnover by 1/4 root transformation"

lambda = 0.25 # 1/4th root transformation

retail |>
  autoplot(box_cox(Turnover, lambda)) +
  labs(y = "1/4 root of Turnover (mil $)",x = "Timeline (in Months)",
       title = latex2exp::TeX(paste0(
         "Transformed annual retailing turnover with $\\lambda$ = ",
         round(lambda,2))),caption = "Source: Australian Bureau of Statistics") + theme_minimal()
```
As observed in @fig-qtr, the __transformation appears to be weak in the towards the early years as compared to the later years.__

In order to increase the effect of the Box-Cox transformation, we will need to reduce the $\lambda$ value further. Let us try using $\lambda = 0.1$.

```{r}
#| label: fig-ten
#| fig-cap: "Transformation of the annual retailing turnover by 10th root transformation"

lambda = 0.1 # 1/10th root transformation

retail |>
  autoplot(box_cox(Turnover, lambda)) +
  labs(y = "1/10 root of Turnover (mil $)",x = "Timeline (in Months)",
       title = latex2exp::TeX(paste0(
         "Transformed annual retailing turnover with $\\lambda$ = ",
         round(lambda,2))),caption = "Source: Australian Bureau of Statistics") + theme_minimal()
```

Based on @fig-ten, it appears that a transformation with $\lambda = 0.1$ has still missed out in the seasonality variation during the early years as there still appears to be some amount of minor differences between the initial seasonality variation and the later seasonality variation.


Due to the inherent low sensitivity of the $\lambda$ parameter, we can consider that any value lower than 0.1 to have the same effect as when $\lambda = 0$. __This means that the Box-Cox transformation approaches towards a logarithmic transformation.__


Let us attempt to visualise this by performing a log transformation through $\lambda = 0$


```{r}
#| label: fig-transform-log
#| fig-cap: "Transformation of the annual retailing turnover by Log transformation"

lambda = 0 # Log transformation

retail |>
  autoplot(box_cox(Turnover, 0)) +
  labs(y = "Log of Turnover (mil $)",x = "Timeline (in Months)",
       title = latex2exp::TeX(paste0(
         "Transformed annual retailing turnover with $\\lambda$ = ",
         round(lambda,2))),caption = "Source: Australian Bureau of Statistics") + theme_minimal()
```

:::{.callout-note}
# Key takeaway

@fig-transform-log illustrates the timeseries data for retailing turnover after being transformed through the Box-Cox transformation on the log scale. Here are some key observations:

1. We observe that unlike the original data, the variation in seasonality of the Log transformed data was much more similar through the timeseries.

2. Using the Log transformation, the datapoints with lower seasonality variation (data in the initial time period) was stretched to larger variations while the datapoints with higher seasonality variation (data in the later time period) was compressed to relatively smaller variations.

3. Since the variations are fairly consistent throughout the entire period of the data, we can consider the transformed data to contain an additive variation of seasonality.

4. Usage of a __log transformation also provides better interpretability as changes in a log value are proportional to changes on the original scale, multiplied by a factor.__

:::

### Automating the estimation of $\lambda$

We can additionally use the __Guerrero transformation__ as a means to automate the process of obtaining a $\lambda$ value. Based on this optimisation technique, a value of $\lambda = -0.09$ was obtained. The corresponding plot upon transformation can be observed in @fig-transform.

```{r}
#| label: fig-transform
#| fig-cap: "Guerrero transformation of the annual retailing turnover data using Box-Cox method"

lambda <- retail |>
  features(Turnover, features = guerrero) |>
  pull(lambda_guerrero)

retail |>
  autoplot(box_cox(Turnover, lambda)) +
  labs(y = "Turnover (mil $)",x = "Timeline (in Months)",
       title = latex2exp::TeX(paste0(
         "Transformed annual retailing turnover with $\\lambda$ = ",
         round(lambda,2))),caption = "Source: Australian Bureau of Statistics") + theme_minimal()
```
:::{.callout-note}
As the transformation parameter $\lambda$ is not very sensitive to small changes, hence, even though the optimised calculation as a result of Guerrero transformation provides us with a value of $\lambda = -0.9$, however, for reasons stated in @sec-transform, we will consider $\lambda = 0$ (Logarithmic transformation) for our analysis.
:::


## Seasonal differencing

Now that we have transformed the data such that the variation of the peaks and troughs are consistent and do not change with the level of the timeseries, now we shall attempt to seasonally difference the data.


```{r}
#| label: fig-seasonaldiff
#| fig-cap: "Seasonal differencing of the timeseries data"

retail %>%
  gg_tsdisplay(
    Turnover %>%
      box_cox(lambda = 0.3) %>%
      difference(lag = 12),
    plot_type = "partial",lag_max = 36) + labs(y = "Annual change in Log(Turnover)") 
```

```{r}
lb_pvalue <- retail %>%
  mutate(diff_turnover = difference(Turnover)) |>
  features(diff_turnover, ljung_box, lag = 10) %>% pull(lb_pvalue)
```



:::{.callout-note}
# Key takeaway

Based on @fig-seasonaldiff, we observe that:

- The timeseries data of the log of turnover after seasonally differencing appears to be non stationary even though centred majorly around 0, with the exception of a few outliers in 2020 due to the effects of COVID-19.

- Based on the ACF plot, we can see that __the autocorrelation at lag 12 (or at end of the year) is still significant__, but becomes insignificant from there on. It nearly takes a full year for the lags to become insignificant.

- From the above analysis, we are not certain whether the ACF plot has decayed exponentially enough. In order to objectively state that a regular differencing is necessary, we rely on the KPSS unit root test.

:::

## KPSS Unit root test

This test allows us to objectively say if result obtained in @fig-seasonaldiff requires further differencing in the form of regular differencing. According to this test, the null hypothesis is as follows:

$H_o$ : The timeseries data is stationary

If the KPSS P-value is 0.01, it means that we need to reject the null hypothesis and perform a regular differencing while a KPSS P-value of 0.1 suggests that no further differencing is necessary.


```{r}
#| label: tbl-kpss
#| tbl-cap: "KPSS unit root test for the timseries data"
retail %>%
  mutate(diff_turnover = difference(Turnover,lag = 12)) |>
  features(diff_turnover, unitroot_kpss) %>% kable(digits = 2, align = "r") %>% 
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```

:::{.callout-note}
For our timeseries data, the KPSS P-value as tabulated in @tbl-kpss is __`r retail %>%  mutate(diff_turnover = difference(Turnover,lag = 12)) |> features(diff_turnover, unitroot_kpss) %>% pull(kpss_pvalue)`__. Hence, we are __not required to perform further differencing of our timeseries data.__

This additionally means that the __seasonal differencing has successfully made our timeseries data stationary in nature.__
:::



# Methodology for creating a list of ARIMA and ETS models

:::{.callout-warning}
# Note

- ETS models designed to handle both trend and seasonality in the data. Hence, __to obtain an ETS model, we will work with the un-transformed retail data.__ 

- The __ARIMA model cannot handle data which varies with time__, which is the case for the retail data. As a result, we will __utilise the stationary data__ worked out in @sec-transform to fit these models. 

- While exponential smoothing models are based on a description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations in the data.

:::

## Splitting into train and test data

As a first step, we will split our data into train and test data. Let us allocate the final 2 years as the test dataset while the remaining data will be utilised as the training data for the models to fit.

```{r}
#| echo: true
#| code-fold: true

retail_train <- retail %>% filter(Month <= yearmonth("2020 Dec"))
retail_test <- retail %>% filter(Month > yearmonth("2020 Dec"))
```


## Creating an ETS model

Based on our analysis of @fig-turnover, we have established that our data contains a linear trend and a multiplicative variation in seasonality. Hence, while fitting the ETS model, we will keep these in mind. We are mainly interested to evaluate the following two models for the current timeseries data:

1. Default ETS model
2. ETS model with multiplicative errors, additive trend and multiplicative seasonality (MAM model)


```{r}
#| echo: true
#| code-fold: true
fit_ets <- retail_train %>%
  model(mam=ETS(Turnover ~ error("M") + trend("A")+ season("M")),
        ets_auto = ETS(Turnover)) 
```

Let us check the default model identified by the ETS algorithm.

```{r}
#| label: tbl-ets
#| tbl-cap: "Fitted ETS models"

fit_ets %>% kbl()
```
```{r}
#| label: tbl-etscomp
#| tbl-cap: "Comparison of ETS models"

glance(fit_ets) %>% select(".model":"BIC") %>% arrange(AIC) %>% kable(digits = 2, align = "r") %>% 
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```


:::{.callout-note}
# Comparison of ETS models

- As we can observe from @tbl-ets, the default ETS model fitted a "MAdM" model which is expected to produce forecasts with damped trends.

- @tbl-etscomp further compares the performance of the two models. Based on the AIC metric, the auto-generated ETS which is a "MAdM" model with damping was found to perform better than the "MAM" model without damping.
:::


## Creating an ARIMA model

An ARIMA model will be fitted on the stationary data which has been obtained in @sec-transform. In order to obtain the specifications of the ARIMA models which will be fitted, we will need to look deeper into the residuals as illustrated by @fig-resid.

```{r}
#| label: fig-resid
#| fig-cap: "Residuals of the stationary retail data"

retail %>%
  gg_tsdisplay(
    Turnover %>%
      box_cox(lambda = 0) %>%
      difference(lag = 12),lag_max=36,
    plot_type = "partial") + labs(y = "Residuals")
```

:::{.callout-note}
# Shortlisting ARIMA models

- We first look at both the ACF and PACF plots to shortlist some of the important ARIMA models. Since our data would have both trend and seasonality, we will be primarily looking into seasonal ARIMA (or SARIMA) models.

- Upon observing the PACF plot, we see that the seasonal lags at the end of each year is always significant. Hence, if we are looking to create a seasonal AR model, this would be quite complicated and may not be entirely accurate. However, if we were to create a model anyhow, the seasonal AR model would be defined as __ARIMA(p=24,d=1,q=0)[P=2,D=1,Q=0] (m=12).__

- To create a seasonal MA model, we analyse the ACF plot and mark the last significant lag. In this case, the last significant lag is observed at the end of 1 year (lag 12). Therefore, the seasonal MA model can be defined as __ARIMA(p=0,d=1,q=12)[P=0,D=1,Q=1] (m = 12)__.

- We can potentially create a mixture of both a seasonal AR and MA models. Some of potential combinations without overstepping the boundaries of each of the above models would be:

__ARIMA(p=24,d=1,q=12)[P=1,D=1,Q=1] (M=12)__ \
__ARIMA(p=24,d=1,q=12)[P=2,D=1,Q=1] (M=12)__ \

- We will additionally let the ARIMA function choose an automatically generated model which it does so by miniming the AIC.
:::

```{r}
fit_arima <- retail_train %>%
  model(arima_auto = ARIMA(box_cox(Turnover,0)),
        ar = ARIMA(box_cox(Turnover,0) ~ 0 + pdq(24,1,0) + PDQ(2,1,0)),
        ma = ARIMA(box_cox(Turnover,0) ~ 0 + pdq(0,1,12) + PDQ(0,1,1)),
        mix = ARIMA(box_cox(Turnover,0) ~ 0 + pdq(24,1,12) + PDQ(2,1,1))
        ) 


retail_train %>%
  model(arima_auto = ARIMA(box_cox(Turnover,0)))
```


```{r}
#| label: tbl-arimacomp
#| tbl-cap: "Comparison of ARIMA models"

glance(fit_arima) %>% select(".model":"BIC") %>% arrange(AIC) %>% kable(digits = 2, align = "r") %>%
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```

:::{.callout-note}
# Comparison of ARIMA models

Among the ARIMA models which were fitted, the auto-generated ARIMA was observed to perform better than the AR and MA models while comparing on the basis of AIC. The fitted statistics can be referred to in @tbl-arimacomp.
:::

:::{.callout-warning}
# Note on AIC comparison for ETS and ARIMA model

__We cannot use AIC for comparing ARIMA and ETS models.__ This is because ARIMA and ETS are __looking at completely different data (transformed and original data respectively) and this is reflected through the significant differences of AICs for the ETS and ARIMA models.__ Hence, the AIC metric can only be used to compare between different ETS models or between ARIMA models which share the same differencing value.

In our case, __all the ARIMA models have a differencing of 1, and hence, can be compared on the basis of AIC.__

RMSE can be however be compared across the different models. This is because during computation of RMSE scores, all transformations on the data are undone and we are simply checking the level of forecast accuracy across different models. 
:::


```{r}
#| label: tbl-forecomp
#| tbl-cap: "Comparison of forecasts for each model"

bind_rows(
    fit_arima |> accuracy(),
    fit_ets |> accuracy(),
    fit_arima |> forecast(h = 24) |> accuracy(retail),
    fit_ets |> forecast(h = 24) |> accuracy(retail)
  ) |> filter(.type == "Test") %>%
  select(-ME, -MPE, -ACF1,-State,-Industry) %>% arrange(RMSE) %>% kable(digits = 2, align = "r") %>%
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```


:::{.callout-note}
# Forecast comparison based on RMSE

We utilise the root mean square metric (RMSE) to compare the forecast accuracy of each model. These values are tabulated in @tbl-forecomp.

Based on the RMSE scores of the 2-year forecast period, we can observe the __the MAM ETS model has performed the best when compared to all the ETS and ARIMA models fitted on the training data.__

:::


# Looking deeper into the selected ARIMA and ETS model

Now that we have shortlisted a range of ETS and seasonal ARIMA models, we will analyse the best ETS and seasonal ARIMA models based on AIC values. In each case, the auto generated ETS and seasonal ARIMA models were observed to be the ones with the lowest AIC metric and are described in @tbl-finalmodel.

```{r}
final_models <- tibble()
final_models <- fit_arima %>% select(arima_auto)
final_models <- final_models %>% bind_cols(fit_ets %>% select(ets_auto))
```

```{r}
#| label: tbl-finalmodel
#| tbl-cap: "Selected ARIMA and ETS models"
final_models %>% kable(digits = 2, align = "r") %>%
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```

## Residual diagnostics

As a first step to analyse the model, we will check whether our assumptions of the residuals in each of these models hold true. __We assume that the model must have residuals which are independent and identically distributed (IID).__

```{r}
#| label: fig-sarimaresid
#| fig-cap: "Residuals of the seasonal ARIMA model"
final_models %>% select(arima_auto) %>% gg_tsresiduals() + labs(title = "Seasonal ARIMA(1,1,1)[0,1,2](12) residual diagnostics")
```

:::{.callout-note}
# Residuals of SARIMA model

Based on the residual plot as illustrated by @fig-sarimaresid, we can observe that 
- the residuals appear as white noise. 
- there are no significant lags in the ACF, suggesting that the residuals are independent and has not missed out on any information.
- the residuals appear to be normally distributed apart from one outlier which is due to the low retail sales observed in the COVID-19 period.

These findings suggest that the assumptions do hold true for the residuals of the SARIMA model.
:::


```{r}
#| label: fig-etsresid
#| fig-cap: "Residuals of the ETS model"
final_models %>% select(ets_auto) %>% gg_tsresiduals() + labs(title = "ETS (MAdM) residual diagnostics") 
```

:::{.callout-note}
# Residuals of ETS model

Based on the residual plot as illustrated by @fig-etsresid, we can observe that 
- the residuals appear as white noise. 
- apart from the first significant lag, all other lags are insignificant,suggesting that the residuals are independent. However, the first significant lag could be of concern here.
- the residuals appear to be normally distributed apart from one outlier which is due to the low retail sales observed in the COVID-19 period.

These findings suggest that the assumptions do hold true for the residuals of the ETS model.
:::


## Ljung-Box test

While we have objectively stated that the residuals appear as white noise in each of the models, however we can rely on a statistical test to confirm our analysis through the Ljung-box test. If the lb_pvalue is higher than 0.05, we cannot reject the null hypothesis which states that the residuals appear as white noise.

```{r}
#| label: tbl-lbpval
#| tbl-cap: "Ljung-Box test results"

lb_pval_arima <- augment(final_models) %>% filter(.model == "arima_auto") %>%
  features(.innov,ljung_box,lag = 36,fit_df = 4)


augment(final_models) %>% filter(.model == "ets_auto") %>%
  features(.innov,ljung_box,lag = 36) %>% bind_rows(lb_pval_arima) %>%
kable(align = "r") %>%
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```

:::{.callout-note}
# Ljunx-Box test results

- To perform the Ljung-Box test for the seasonal ARIMA model, we need to choose the __degree of freedom to compare with the appropriate null distribution__. The degree of freedom is calculated as follows:

$$ \boxed{DOF = p + q + P + Q} $$ {#eq-df}

Based on @eq-df, the __degree of freedom for the SARIMA(1,1,1)[0,1,2] (m=12) will be 4.__ 

- The Ljung-Box test results tabulated in @tbl-lbpval suggests that while the __residuals of the seasonal ARIMA model are insignificant, the ETS model however has significant residuals, possibly due to the significant lag observed at lag 1.__



:::


