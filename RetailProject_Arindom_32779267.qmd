---
title: "Retail forecasting project"
author: "Arindom Baruah (32779267)"
format: html
execute: 
  echo: false
  message: false
  warning: false
number-sections: true
---

```{r}
library(fpp3)
library(tidyverse)
library(kableExtra)
library(latex2exp)
library(plotly)
library(readabs)
```



```{r}
get_my_data <- function(student_id) {
  set.seed(student_id)
  all_data <- fill_gaps(readr::read_rds("https://bit.ly/monashretaildata"))
  while(TRUE) {
    retail <- filter(all_data, `Series ID` == sample(`Series ID`, 1)) 
    if(!any(is.na(retail$Turnover))) return(retail)
  }
}
# Replace the argument with your student ID
retail <- get_my_data(32779267)

```


# Statistical features of the data


## Complete timeseries data {#sec-overall}

Let us observe the time series data for the turnover generated.

```{r}
#| label: fig-turnover
#| fig-cap: "Footwear and personal accessory retailing turnover \n in Western Australia over time"
retail %>% autoplot(Turnover) + labs(x = "Timeline (in Months)",y = "Turnover (mil $)",title="Footwear and personal accessory retailing turnover \n in Western Australia over time",caption = "Source: Australian Bureau of Statistics") + geom_point(data = NULL, aes(x = "2020-02-15", y = 50), color = "red", shape = 1, size = 7) + theme_minimal()


```
:::{.callout-note}
# Key takeaway

Based on @fig-turnover we can observe the following details in the timeseries plot:

1. There appears to be a seasonality in the plot with a peak being observed on every December of a year. __The seasonality additionally appears to be consistent throughout the plot.__

2. The amplitude of the peaks observed in the data appear to be increasing with time. __This indicates that the variation in seasonality of the data is multiplicative in nature.__

3. The plot also indicates the presence of an __additive trend__ in the data with overall turnover increasing every year except for the year of 2020 in an __approximate linear fashion.__ 

4. There appears to be an __outlier in the data during the year of 2020__ as indicated by the <span style=color:red>red cricle</span>. While we expect a drop in retailing turnover in the early months of a year due to a strong retailing period in the previous December, however, __in the year of 2020, the drop in turnover was considerably lower than all the other years.__ This is expected to be caused due to __shutting down of businesses and their activities in the initial part of the COVID-19 pandemic.__
:::

## Variations of trade within a year {#sec-season}



```{r}
#| label: fig-seasonplot
#| fig-cap: "Variation of footwear and personal accessory retailing turnover \n within a year in Western Australia"


ggplotly(retail %>% gg_season(Turnover) + labs(x = "Month",y = "Turnover (mil $)",title="Variation of footwear and personal accessory retailing turnover \n within a year in Western Australia",caption = "Source: Australian Bureau of Statistics")  + theme_minimal(),tooltip = c("Turnover","Month"))

```
:::{.callout-note} 
# Key takeaway
@fig-seasonplot illustrates the variation of turnover within each year generated by the retailers in Western Australia each year. Some key observations are as follows:

1. The turnover generated in Western Australia has been on a rise each year. This suggests that the expenditure of people on footwear and personal accessory retailing services in WA have been increasing each year and could be attributed to the growing population in the state. 

2. We observe that the retailing turnovers in each year are __generally higher in the months of November and December__. This could be a result of the festive period when there are many customers purchasing gifts for families and friends or due to the __special sales promotions such as Boxing Day sales.__

3. Additionally, the month of November also includes the __Black Friday sale__ which attracts multiple customers to purchase items __due to specialised and limited time promotions.__ This may explain the slight rise in turnover during the month of November each year.

4. The __rise in the retail turnovers for the month of December is especially higher during the more recent years (2011-2021)__. This could again be an indicator for the growing population in the country during this period, which has gone on to to create a bigger market and drive sales, hence, contributing to the higher turnover.

5. There appears to be __one single year when the retail turnover dropped steeply in the month of April but eventually picked up for the rest of the year.__ This could be an indicator of the effects of the COVID-19 lockdown when the business activities were very limited and prevented multiple retailers in various regions of Australia, including the state of Western Australia to be able to generate revenue.

:::

## Comparison of turnovers in each month of retail trade

```{r}
#| label: fig-subseries
#| fig-cap: "Variation of footwear and personal accessory retailing turnover in each month of a year in Australia"

retail %>% gg_subseries(Turnover) + labs(x = "Year",y = "Turnover (mil $)",title="Variation of footwear and personal accessory retailing turnover \n in each month of a year in Western Australia",caption = "Source: Australian Bureau of Statistics")  
```

:::{.callout-note}
# Key takeaway

@fig-subseries illustrates the variation of retailing turnover faceted by each month and plotted against years. Key observations are as follows:

1. As already observed in @sec-season, the plot suggests that the __retailing turnover has increased each year from 1990 to 2022.__

2. The average __retail turnover is observed to be higher for the month of December when compared to the rest of the year.__ As explained previously, the main driver for the boost of retailing turnover in this period is attributed to the festive season which includes __Christmas, Boxing Day and the run up to the New Year Eve.__

3. A __drop in retail turnover for the months of March, April and May was observed in 2020.__ This can be attributed to the lockdowns set in place when the pandemic outbreak was first experienced.
:::

# Transformation and differencing of the data {#sec-transform}


In @sec-overall, we have determined that the timeseries data in hand has multiplicative variation in seasonality and a linear trend. This inadvertantly means that we are dealing with non-stationary data.

Converting a non-stationary data to a stationary data is done through the following steps:

## Box-Cox Transformation

In order to apply the Box-Cox transformation, we are required to obtain the appropriate value of the parameter $\lambda$ which will allow us to analyse the timeseries data with the seasonal variations being equal at all levels, thereby transforming the multiplicative variation in the data into an additive variation.

### Estimating the value of $\lambda$ manually {#sec-manual}

In order to apply the Box-Cox transformation, we are required to obtain the appropriate value of the parameter $\lambda$ which will allow us to analyse the timeseries data with the seasonal variations being equal at all levels, thereby transforming the multiplicative variation in the data into an additive variation.

```{r}
#| label: fig-sqrt
#| fig-cap: "Transformation of the annual retailing turnover by square root transformation"
lambda = 0.5 # Square root transformation

retail |>
  autoplot(box_cox(Turnover, lambda)) +
  labs(y = "Square root of Turnover (mil $)",x = "Timeline (in Months)",
       title = latex2exp::TeX(paste0(
         "Transformed annual retailing turnover with $\\lambda$ = ",
         round(lambda,2))),
       caption = "Source: Australian Bureau of Statistics")  + theme_minimal()
```
@fig-sqrt illustrates a square root transformation. As we can observe, the __square root transformation appears to be weak in nature__, and hence, not be able to effectively make the variations in seasonality equal.


Let us now attempt to transform the data using $\lambda = 0.25$

```{r}
#| label: fig-qtr
#| fig-cap: "Transformation of the annual retailing turnover by 1/4 root transformation"

lambda = 0.25 # 1/4th root transformation

retail |>
  autoplot(box_cox(Turnover, lambda)) +
  labs(y = "1/4 root of Turnover (mil $)",x = "Timeline (in Months)",
       title = latex2exp::TeX(paste0(
         "Transformed annual retailing turnover with $\\lambda$ = ",
         round(lambda,2))),caption = "Source: Australian Bureau of Statistics") + theme_minimal()
```
As observed in @fig-qtr, the __transformation appears to be weak in the towards the early years as compared to the later years.__

In order to increase the effect of the Box-Cox transformation, we will need to reduce the $\lambda$ value further. Let us try using $\lambda = 0.1$.

```{r}
#| label: fig-ten
#| fig-cap: "Transformation of the annual retailing turnover by 10th root transformation"

lambda = 0.1 # 1/10th root transformation

retail |>
  autoplot(box_cox(Turnover, lambda)) +
  labs(y = "1/10 root of Turnover (mil $)",x = "Timeline (in Months)",
       title = latex2exp::TeX(paste0(
         "Transformed annual retailing turnover with $\\lambda$ = ",
         round(lambda,2))),caption = "Source: Australian Bureau of Statistics") + theme_minimal()
```

Based on @fig-ten, it appears that a transformation with $\lambda = 0.1$ has still missed out in the seasonality variation during the early years as there still appears to be some amount of minor differences between the initial seasonality variation and the later seasonality variation.


Due to the inherent low sensitivity of the $\lambda$ parameter, we can consider that any value lower than 0.1 to have the same effect as when $\lambda = 0$. __This means that the Box-Cox transformation approaches towards a logarithmic transformation.__


Let us attempt to visualise this by performing a log transformation through $\lambda = 0$


```{r}
#| label: fig-transform-log
#| fig-cap: "Transformation of the annual retailing turnover by Log transformation"

lambda = 0 # Log transformation

retail |>
  autoplot(box_cox(Turnover, 0)) +
  labs(y = "Log of Turnover (mil $)",x = "Timeline (in Months)",
       title = latex2exp::TeX(paste0(
         "Transformed annual retailing turnover with $\\lambda$ = ",
         round(lambda,2))),caption = "Source: Australian Bureau of Statistics") + theme_minimal()
```

:::{.callout-note}
# Key takeaway

@fig-transform-log illustrates the timeseries data for retailing turnover after being transformed through the Box-Cox transformation on the log scale. Here are some key observations:

1. We observe that unlike the original data, the variation in seasonality of the Log transformed data was much more similar through the timeseries.

2. Using the Log transformation, the datapoints with lower seasonality variation (data in the initial time period) was stretched to larger variations while the datapoints with higher seasonality variation (data in the later time period) was compressed to relatively smaller variations.

3. Since the variations are fairly consistent throughout the entire period of the data, we can consider the transformed data to contain an additive variation of seasonality.

4. Usage of a __log transformation also provides better interpretability as changes in a log value are proportional to changes on the original scale, multiplied by a factor.__

:::

### Automating the estimation of $\lambda$

We can additionally use the __Guerrero transformation__ as a means to automate the process of obtaining a $\lambda$ value. Based on this optimisation technique, a value of $\lambda = -0.09$ was obtained. The corresponding plot upon transformation can be observed in @fig-transform.

```{r}
#| label: fig-transform
#| fig-cap: "Guerrero transformation of the annual retailing turnover data using Box-Cox method"

lambda <- retail |>
  features(Turnover, features = guerrero) |>
  pull(lambda_guerrero)

retail |>
  autoplot(box_cox(Turnover, lambda)) +
  labs(y = "Turnover (mil $)",x = "Timeline (in Months)",
       title = latex2exp::TeX(paste0(
         "Transformed annual retailing turnover with $\\lambda$ = ",
         round(lambda,2))),caption = "Source: Australian Bureau of Statistics") + theme_minimal()
```
:::{.callout-note}
As the transformation parameter $\lambda$ is not very sensitive to small changes, hence, even though the optimised calculation as a result of Guerrero transformation provides us with a value of $\lambda = -0.9$, however, for reasons stated in @sec-transform, we will consider $\lambda = 0$ (Logarithmic transformation) for our analysis.
:::


## Seasonal differencing

Now that we have transformed the data such that the variation of the peaks and troughs are consistent and do not change with the level of the timeseries, now we shall attempt to seasonally difference the data.


```{r}
#| label: fig-seasonaldiff
#| fig-cap: "Seasonal differencing of the timeseries data"

retail %>%
  gg_tsdisplay(
    Turnover %>%
      box_cox(lambda = 0.3) %>%
      difference(lag = 12),
    plot_type = "partial",lag_max = 36) + labs(y = "Annual change in Log(Turnover)") 
```

```{r}
lb_pvalue <- retail %>%
  mutate(diff_turnover = difference(Turnover)) |>
  features(diff_turnover, ljung_box, lag = 10) %>% pull(lb_pvalue)
```



:::{.callout-note}
# Key takeaway

Based on @fig-seasonaldiff, we observe that:

- The timeseries data of the log of turnover after seasonally differencing appears to be non stationary even though centred majorly around 0, with the exception of a few outliers in 2020 due to the effects of COVID-19.

- Based on the ACF plot, we can see that __the autocorrelation at lag 12 (or at end of the year) is still significant__, but becomes insignificant from there on. It nearly takes a full year for the lags to become insignificant.

- From the above analysis, we are not certain whether the ACF plot has decayed exponentially enough. In order to objectively state that a regular differencing is necessary, we rely on the KPSS unit root test.

:::

## KPSS Unit root test

This test allows us to objectively say if result obtained in @fig-seasonaldiff requires further differencing in the form of regular differencing. According to this test, the null hypothesis is as follows:

$H_o$ : The timeseries data is stationary

If the KPSS P-value is 0.01, it means that we need to reject the null hypothesis and perform a regular differencing while a KPSS P-value of 0.1 suggests that no further differencing is necessary.


```{r}
#| label: tbl-kpss
#| tbl-cap: "KPSS unit root test for the timseries data"
retail %>%
  mutate(diff_turnover = difference(Turnover,lag = 12)) |>
  features(diff_turnover, unitroot_kpss) %>% kable(digits = 2, align = "r") %>% 
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```

:::{.callout-note}
For our timeseries data, the KPSS P-value as tabulated in @tbl-kpss is __`r retail %>%  mutate(diff_turnover = difference(Turnover,lag = 12)) |> features(diff_turnover, unitroot_kpss) %>% pull(kpss_pvalue)`__. Hence, we are __not required to perform further differencing of our timeseries data.__

This additionally means that the __seasonal differencing has successfully made our timeseries data stationary in nature.__
:::



# Methodology for creating a list of ARIMA and ETS models

:::{.callout-warning}
# Note

- ETS models designed to handle both trend and seasonality in the data. Hence, __to obtain an ETS model, we will work with the un-transformed retail data.__ 

- The __ARIMA model cannot handle data which varies with time__, which is the case for the retail data. As a result, we will __utilise the stationary data__ worked out in @sec-transform to fit these models. 

- While exponential smoothing models are based on a description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations in the data.

:::

## Splitting into train and test data

As a first step, we will split our data into train and test data. Let us allocate the final 2 years as the test dataset while the remaining data will be utilised as the training data for the models to fit.

```{r}
#| echo: true
#| code-fold: true

retail_train <- retail %>% filter(Month <= yearmonth("2020 Dec"))
retail_test <- retail %>% filter(Month > yearmonth("2020 Dec"))
```


## Creating an ETS model

Based on our analysis of @fig-turnover and our decomposition of the data, we have established that our data contains a linear trend and a multiplicative variation in seasonality. Hence, while fitting the ETS model, we will keep these in mind. We are mainly interested to evaluate the following two models for the current timeseries data:

1. Default ETS model
2. ETS model with multiplicative errors, additive trend and multiplicative seasonality (MAM model)




```{r}
#| echo: true
#| code-fold: true
fit_ets <- retail_train %>%
  model(mam=ETS(Turnover ~ error("M") + trend("A")+ season("M")),
        ets_auto = ETS(Turnover)) 
```

```{r}
#| label: fig-decomp
#| fig-cap: "Decomposition of the ETS model"

fit_ets %>% select(mam) %>% components() %>% 
autoplot() + labs(title = "Decomposition plot",subtitle = "ETS (M,A,M) model")
```

Let us check the default model identified by the ETS algorithm.

```{r}
#| label: tbl-ets
#| tbl-cap: "Fitted ETS models"

fit_ets %>% kbl()
```
```{r}
#| label: tbl-etscomp
#| tbl-cap: "Comparison of ETS models"

glance(fit_ets) %>% select(".model":"BIC") %>% arrange(AIC) %>% kable(digits = 2, align = "r") %>% 
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```


:::{.callout-note}
# Comparison of ETS models based on AIC

- As we can observe from @tbl-ets, the default ETS model fitted a "MAdM" model which is expected to produce forecasts with damped trends.

- @tbl-etscomp further compares the performance of the two models. Based on the AIC metric, the auto-generated ETS which is a "MAdM" model with damping was found to perform better than the "MAM" model without damping.
:::


## Creating an ARIMA model

An ARIMA model will be fitted on the stationary data which has been obtained in @sec-transform. In order to obtain the specifications of the ARIMA models which will be fitted, we will need to look deeper into the residuals as illustrated by @fig-resid.

```{r}
#| label: fig-resid
#| fig-cap: "Residuals of the stationary retail data"

retail %>%
  gg_tsdisplay(
    Turnover %>%
      box_cox(lambda = 0) %>%
      difference(lag = 12),lag_max=36,
    plot_type = "partial") + labs(y = "Residuals")
```

:::{.callout-note}
# Shortlisting ARIMA models

- We first look at both the ACF and PACF plots to shortlist some of the important ARIMA models. Since our data would have both trend and seasonality, we will be primarily looking into seasonal ARIMA (or SARIMA) models.

- Upon observing the PACF plot, we see that the seasonal lags at the end of each year is always significant. Hence, if we are looking to create a seasonal AR model, this would be quite complicated and may not be entirely accurate. However, if we were to create a model anyhow, the seasonal AR model would be defined as __ARIMA(p=24,d=0,q=0)[P=2,D=1,Q=0] (m=12).__

- To create a seasonal MA model, we analyse the ACF plot and mark the last significant lag. In this case, the last significant lag is observed at the end of 1 year (lag 12). Moreover, as we have already accounted for the seasonal lag, we will choose another lag which is the next highest after lag 12. In this instance, we choose our regular lag as lag 9. This indicates that our seasonal MA model will be defined as __ARIMA(p=0,d=1,q=9)[P=0,D=1,Q=1] (m=12)__.

- We can potentially create a mixture of both a seasonal AR and MA models. Some of potential combinations without overstepping the boundaries of each of the above models would be:

__ARIMA(p=24,d=0,q=9)[P=1,D=1,Q=1] (M=12)__ \
__ARIMA(p=24,d=0,q=9)[P=2,D=1,Q=1] (M=12)__ \

- We will additionally let the ARIMA function choose an automatically generated model which it does so by minimising the AIC.
:::

```{r}
fit_arima <- retail_train %>%
  model(arima_auto = ARIMA(box_cox(Turnover,0)),
        ar = ARIMA(box_cox(Turnover,0) ~ 0 + pdq(24,0,0) + PDQ(2,1,0)),
        ma = ARIMA(box_cox(Turnover,0) ~ 0 + pdq(0,0,9) + PDQ(0,1,1)),
        mix = ARIMA(box_cox(Turnover,0) ~ 0 + pdq(24,0,9) + PDQ(2,1,1))
        ) 

```


```{r}
#| label: tbl-arimacomp
#| tbl-cap: "Comparison of ARIMA models"

glance(fit_arima) %>% select(".model":"BIC") %>% arrange(AIC) %>% kable(digits = 2, align = "r") %>%
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```

:::{.callout-note}
# Comparison of ARIMA models

Among the ARIMA models which were fitted, the auto-generated ARIMA was observed to perform better than the AR and MA models while comparing on the basis of AIC. The fitted statistics can be referred to in @tbl-arimacomp.
:::

:::{.callout-warning}
# Note on AIC comparison for ETS and ARIMA model

__We cannot use AIC for comparing ARIMA and ETS models.__ This is because ARIMA and ETS are __looking at completely different data (transformed and original data respectively) and this is reflected through the significant differences of AICs for the ETS and ARIMA models.__ Since the __log of likelihood is expected to differ for both the untransformed data (for ETS model) and transformed data (for ARIMA model)__, and AIC metric is based on minimising this log likelihood value, hence, comparing the metric for these two sets of models will not provide us with a meaningful insight into their performances.

In our case, __all the ARIMA models have a differencing of 1, and hence, can be compared on the basis of AIC.__

RMSE can be however be compared across the different models. This is because during computation of RMSE scores, all transformations on the data are undone and we are simply checking the level of forecast accuracy across different models. 
:::


```{r}
#| label: tbl-forecomp
#| tbl-cap: "Comparison of forecasts for each model"

bind_rows(
    fit_arima |> accuracy(),
    fit_ets |> accuracy(),
    fit_arima |> forecast(h = 24) |> accuracy(retail),
    fit_ets |> forecast(h = 24) |> accuracy(retail)
  ) |> filter(.type == "Test") %>%
  select(-ME, -MPE, -ACF1,-State,-Industry,-.type) %>% arrange(RMSE) %>% kable(digits = 2, align = "r") %>%
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```


:::{.callout-note}
# Forecast comparison based on RMSE

We utilise the root mean square metric (RMSE) to compare the forecast accuracy of each model. These values are tabulated in @tbl-forecomp.

Based on the RMSE scores of the 2-year forecast period, we can observe the __the MAM ETS model has performed the best when compared to all the ETS and ARIMA models fitted on the training data.__

:::


# Looking deeper into the selected ARIMA and ETS model

Now that we have shortlisted a range of ETS and seasonal ARIMA models, we will analyse the best ETS and seasonal ARIMA models based on AIC and RMSE values. In each case, the auto generated seasonal ARIMA models were observed to be the ones with the lowest AIC metric and are described in @tbl-finalmodel.

However, for the ETS models, we find that the auto generated ETS model was the one with lowest AIC while the MAM model was the one with the lowest RMSE. __As population is a strong indicator of retail turnover, and we expect the population and the economy to be on the rise for the next 2 years and beyond, hence, we will select a model with an additive trend rather than a damped trend.__ Therefore, we will use the MAM ETS model for our future forecasts.

```{r}
final_models <- tibble()
final_models <- fit_arima %>% select(arima_auto)
final_models <- final_models %>% bind_cols(fit_ets %>% select(mam))
```

```{r}
#| label: tbl-finalmodel
#| tbl-cap: "Selected ARIMA and ETS models"
final_models %>% kable(digits = 2, align = "r") %>%
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```
## Model parameter estimates

Each of the selected ETS and SARIMA models can be described through the model parameter estimates. These parameter estimates for the models are delineated below.

### ETS model parameter estimates

<div class="alert alert-block alert-info">
$\alpha =$ Smoothing parameter for error in the data \
$\beta =$ Smoothing parameter for the trend component of the data \
$\gamma =$ Smoothing parameter for the seasonal component of the data \

</div>

```{r}
final_models %>% select(mam) %>% report()
```


Based on the reported parameter estimates of the ETS (MAM) model, we observe that $\beta \approx 0$. __This suggests that the model does not respond quickly to recent trend changes and weighs the historical trend higher.__

### ARIMA model parameter estimates

<div class="alert alert-block alert-info">

$\phi =$ Coefficients of the autoregressive (AR) parts of the ARIMA model. These coefficients indicate whether there is a positive or a negative correlation between a current value and its previous lags. \

$\theta =$ Coefficients of the moving average (MA) parts of the ARIMA model. These coefficients indicate the positive or negative correlation between the current and past errors. \

</div>

The parameter estimates for the SARIMA(1,1,1)(0,1,2) [12] are as follows:

```{r}
final_models %>% select(arima_auto) %>% report()
```


## Residual diagnostics

As a first step to analyse the model, we will check whether our assumptions of the residuals in each of these models hold true. __We assume that the model must have residuals which are independent and identically distributed (IID).__

```{r}
#| label: fig-sarimaresid
#| fig-cap: "Residuals of the seasonal ARIMA model"
final_models %>% select(arima_auto) %>% gg_tsresiduals() + labs(title = "Seasonal ARIMA(1,1,1)[0,1,2](12) residual diagnostics")
```

:::{.callout-note}
# Residuals of SARIMA model

Based on the residual plot as illustrated by @fig-sarimaresid, we can observe that:

- the __residuals appear as white noise.__ 
- there are no significant lags in the ACF, suggesting that the residuals are independent and has not missed out on any information.
- the residuals appear to be normally distributed apart from one outlier which is due to the low retail sales observed in the COVID-19 period.
- the residual plot indicates that its distribution equally dispersed about 0 and hence, identically distributed.

These findings suggest that the assumptions do hold true for the residuals of the SARIMA model.
:::


```{r}
#| label: fig-etsresid
#| fig-cap: "Residuals of the ETS model"
final_models %>% select(mam) %>% gg_tsresiduals() + labs(title = "ETS (MAM) residual diagnostics") 
```



:::{.callout-note}
# Residuals of ETS model

Based on the residual plot as illustrated by @fig-etsresid, we can observe that: 

- the __residuals do not appear as white noise due to a significant first lag__. Lag 4 is a fairly significant lag to be considered aswell. 
- the variation of the residuals appear to be homoscedastic based on the spread of the innovation residuals across the mean of 0.
- the residuals appear to be normally distributed apart from one outlier which is due to the low retail sales observed in the COVID-19 period.

These findings suggest that the residuals of the ETS model do not satisfy the IID assumptions since not all the auto-correlations appear to be independent. However, this does not necessarily mean that the forecasts of the ETS model are unreliable.
:::


## Ljung-Box test

While we have objectively stated that the residuals appear as white noise in each of the models, however we can rely on a statistical test to confirm our analysis through the Ljung-box test. If the lb_pvalue is higher than 0.05, we cannot reject the null hypothesis which states that the residuals appear as white noise.

```{r}
#| label: tbl-lbpval
#| tbl-cap: "Ljung-Box test results"

lb_pval_arima <- augment(final_models) %>% filter(.model == "arima_auto") %>%
  features(.innov,ljung_box,lag = 36,fit_df = 4)


augment(final_models) %>% filter(.model == "mam") %>%
  features(.innov,ljung_box,lag = 36) %>% bind_rows(lb_pval_arima) %>%
kable(align = "r") %>%
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```

:::{.callout-note}
# Ljunx-Box test results

- To perform the Ljung-Box test for the seasonal ARIMA model, we need to choose the __degree of freedom to compare with the appropriate null distribution__. The degree of freedom is calculated as follows:

$$ \boxed{DOF = p + q + P + Q} $$ {#eq-df}

Based on @eq-df, the __degree of freedom for the SARIMA(1,1,1)[0,1,2] (m=12) will be 4.__ 

- The Ljung-Box test results tabulated in @tbl-lbpval suggests that while the __residuals of the seasonal ARIMA model are insignificant, the ETS model however has significant residuals, possibly due to the significant lag observed at lag 1.__


:::

## Forecasts and prediction intervals 

Now that we have analysed the functioning of each of the models, we will assess the forecasts made by them along with their prediction intervals.


```{r}
fit_final <- retail_train %>% model(ets_mam = ETS(Turnover ~ error("M") + trend("A")+ season("M")),
                             arima_auto = ARIMA(box_cox(Turnover,0)))

```


```{r}
#| label: fig-forecast
#| fig-cap: "Forecasts of each model along with prediction intervals"

fit_final %>% forecast(h = "2 years") %>% autoplot(alpha = 0.4) + geom_line(
    data = retail_train |> tail(120),
    aes(x = Month, y = Turnover, linetype = "Turnover (in mil$)")) + guides(
    color = guide_legend(title = "Forecast model"),
    fill = guide_legend(title = "Forecast model"),
    linetype = guide_legend("ABS data")) + labs(title = "Forecasts for ETS and SARIMA models",
y = "Turnover (in mil $)",x = "Timeline",caption = "Source: Australian Bureau of Statistics") + theme_minimal()

```
```{r}
#| label: tbl-etspred
#| tbl-cap: "Glimpse of the ETS forecasts and its intervals"
fit_final %>% forecast(h = "2 years") %>% hilo() %>% filter(.model == "ets_mam") %>% select(-c(State,Industry,.model)) %>% head() %>%
kable(align = "r") %>% 
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```

```{r}
#| label: tbl-arimapred
#| tbl-cap: "Glimpse of the SARIMA forecasts and its intervals"
fit_final %>% forecast(h = "2 years") %>% hilo() %>% filter(.model == "arima_auto") %>% select(-c(State,Industry,.model)) %>% head() %>%
kable(align = "r") %>% 
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```

:::{.callout-note}
# Key takeaway

@fig-forecast illustrates the forecasts alongwith their prediction intervals for the SARIMA and the ETS model. Some of the key observations are:

- __Both the models are observed to capture the seasonality effect__ of the retail data towards the end of each year.
- Between 2021 and 2027, the SARIMA model consistently forecasted lower means compared to the ETS model. However, __post-2027, there's a noticeable shift, with the SARIMA model indicating higher mean values than the ETS model.__
- Between 2021 to 2026, the SARIMA model was observed to possess a narrower forecast interval. However, __after 2026, the forecast intervals for the SARIMA model are consistently wider than the ETS model.__
:::


# Comparing the forecasts for the ETS and SARIMA models

In order to comment on the forecasts for each of the preferred models, we will compare the forecasted results of the models to the actual data. Let us overlay the predictions with the actual retail numbers as illustrated by @fig-forecompviz.

```{r}
#| label: fig-forecompviz
#| fig-cap: "Forecasts of each model overlayed on the actual turnover"

fit_final %>% forecast(h = "2 years") %>% autoplot() + geom_line(
    data = retail |> tail(120),
    aes(x = Month, y = Turnover, linetype = "Turnover (in mil$)")) + guides(
    color = guide_legend(title = "Forecast model"),
    fill = guide_legend(title = "Forecast model"),
    linetype = guide_legend("ABS data")) + labs(title = "Forecasts for ETS and SARIMA models",
y = "Turnover (in mil $)",x = "Timeline",caption = "Source: Australian Bureau of Statistics") + theme_minimal()



```

:::{.callout-note}
# Comparison of forecasts for ETS and SARIMA model 

- Within the 2 year forecast period, we can observe that the SARIMA model consistently under-forecasted when compared to the ETS model. This was also the case when the actual turnovers were overlayed on the forecasts of the models.

- We can observe that the __point forecasts of the ETS model were closer to the actual values when compared to the SARIMA model point forecasts.__
- The forecasted peak for the ETS model was nearly coincident with the actual peak observed in 2022 and 2023 while the SARIMA model under-forecasted on both the occassions.
- The __actual turnovers in 2022 and 2023 were observed to be always within the 80 % and 95 % prediction intervals for the ETS model__. However, the __forecasts and the prediction intervals based on the SARIMA model were significantly off the mark for the majority of the 2 year forecasts when compared to the actual numbers.__
- Based on the visualisation in @fig-forecompviz and the RMSE results in @tbl-forecomp, we can say that __the ETS (MAM) model forecasted better results when compared to the SARIMA(1,1,1)[0,1,2] (m=12) model.__

:::


# Fitting the models on the entire data

Now that we have fitted the models on the training data and tested it on the in-sample test set, we now fit the two models on the entire ABS retail data and check our foercasts against the out of sample dataset.


```{r}

fit_new <- retail %>% model(ets_mam = ETS(Turnover ~ error("M") + trend("A")+ season("M")),
                            arima_auto = ARIMA(box_cox(Turnover,0) ~ pdq(1,1,1) + PDQ(0,1,2)))
```



```{r}
#| label: tbl-new
#| tbl-cap: "Updated forecast models after fitting on entire dataset"
fit_new %>% kable(align = "r") %>%
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 

```

Let us now check the model estimates after refitting the models on to the entire data.

## ETS Model parameter estimates 


```{r}
fit_new %>% select(ets_mam) %>% report()
```
## SARIMA Model parameter estimates 

```{r}
fit_new %>% select(arima_auto) %>% report()
```
We can additionally express the SARIMA model through the backshift notation as illustrated by @eq-sarima.

<div class="alert alert-block alert-info">

$$\boxed{(1-\phi_1B)(1-B)^1 \times y_t^* = C + (1 + \theta_1 B)(1 + \Theta_1 B^{12} + \Theta_2 B^{24})} $$ {#eq-sarima}
Where,

$y_t^* = {BoxCox}(y_t, \lambda = 0)$ 

$B$ is the backshift notation and is defined as 

$B y_t = y_{t-1}$

Based on the model parameter estimations, the estimates in parameter estimates in @eq-sarima are as follows:

$\phi_1 = 0.4394$ \
$\theta_1 = -0.8348$ \
$\Theta_1 = -0.7907$ \
$\Theta_2 = -0.0736$ \

</div>

:::{.callout-note}
# Fitting ETS and SARIMA on the full dataset

Upon fitting the ETS and SARIMA models with the entire ABS retail dataset, we obtain the following models:

1. The ETS model continues to utilise the MAM model, which considers multiplicative errors, additive trend and multiplicative seasonality.
2. The SARIMA model has been defined as ARIMA(1,1,1)(0,1,2)[12] to keep it consistent with the initial SARIMA model fitted on the training data.

:::

```{r}
#| label: fig-forecastnew
#| fig-cap: "Forecasts of each model along with prediction intervals"

fc_trends <- fit_new %>% forecast(h = "2 years") 


autoplot(fc_trends, alpha = 0.8,level = 80) +
  geom_line(
    data = retail |> tail(120),
    aes(x = Month, y = Turnover, linetype = "Turnover (in mil$)")
  ) +
  guides(
    color = guide_legend(title = "Forecast model"),
    fill = guide_legend(title = "Forecast model"),
    linetype = guide_legend("ABS data")
  ) +
  labs(
    level = "Level",
    x = "Timeline",
    y = "Turnover (in mil $)",
    title = "Forecast prediction comparison with the ABS released data",
    subtitle = "Latest ABS release: March 2024",
    caption = "Source: Australian Bureau of Statistics"
  )  + 
  theme_minimal() 

```
```{r}
#| label: tbl-etsprednew
#| tbl-cap: "Glimpse of the ETS forecasts and its intervals for the recent released ABS data"
fit_new %>% forecast(h = "2 years") %>% hilo() %>% filter(.model == "ets_mam") %>% select(-c(State,Industry,.model)) %>% head() %>%
kable(align = "r") %>% 
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```

```{r}
#| label: tbl-sarimaprednew
#| tbl-cap: "Glimpse of the SARIMA forecasts and its intervals for the recent released ABS data"
fit_new %>% forecast(h = "2 years") %>% hilo() %>% filter(.model == "arima_auto") %>% select(-c(State,Industry,.model)) %>% head() %>%
kable(align = "r") %>% 
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```

:::{.callout-note}
# Key takeaway

Based on the retail forecasts for the years 2023 and 2024 as illustrated by @fig-forecastnew and tabulated in tbl-etsprednew as well as tbl-sarimaprednew, we can make the following observations:

- The point estimates of the ETS and SARIMA model are close to each other in 2023 while the SARIMA model over forecasted in comparison to the ETS model in 2024.

- The 80% prediction intervals are slightly wider for the SARIMA model compared to the ETS model, __indicating relatively higher uncertainty associated with forecasting using the SARIMA model.__

:::

# Obtaining the updated ABS data and assessing forecasts

The latest release of the data from the Australian Bureau of Statistics is obtained through the link [here](https://www.abs.gov.au/statistics/industry/retail-and-wholesale-trade/retail-trade-australia/mar-2024) and will be further cleaned to create our forecasts.


```{r}
#| echo: true
#| code-fold: true

# Obtaining the latest release of ABS data for series ID: A3349825J

abs_data <-  read_abs(
      cat_no = NULL,
      tables = 11,
      series_id = "A3349825J",
      path = Sys.getenv("R_READABS_PATH", unset = tempdir()),
      metadata = TRUE,
      show_progress_bars = TRUE,
      retain_files = TRUE,
      check_local = TRUE,
      release_date = "latest"
)

```


```{r}
#| echo: true
#| code-fold: true

####### Data cleaning process to streamline format of data ##########

retail_data_updated <- abs_data %>% select(series,series_id,date,value)

# Split column with series details into state and industry

col_split <- str_split_fixed(retail_data_updated$series, ";", 3)
retail_data_updated <- data.frame(retail_data_updated, 
                 Type = col_split[, 1], 
                 State = col_split[, 2],
                 Industry = col_split[, 3])

# Remove all unnecessary characters

retail_data_updated$Industry <- str_replace_all(retail_data_updated$Industry, ";", "")

# Rename the fields

retail_data_updated <- retail_data_updated %>% select(State,Industry,series_id,date,value) %>% rename("Turnover" = "value")



# Save dataframe into a variable and create an index as yearmonth

retail_data_updated <- retail_data_updated %>% 
                       select(c(State,Industry,series_id,date,Turnover)) %>% 
                       mutate(Month = yearmonth(date)) %>%
                       as_tsibble(key = c(State,Industry),index = Month)

# Remove all unnecessary whitespaces

retail_data_updated <- retail_data_updated %>%
  mutate(across(State:Industry, ~ str_trim(.x)))


# Finalise the tsibble

retail_data_updated <- retail_data_updated %>% select(State,Industry,series_id,Month,Turnover)

```


Now that we have obtained the latest released data from ABS, we will see how our 2-year forecasts compare against the actual retail turnovers.


```{r}
#| label: fig-forecastnewplot
#| fig-cap: "Forecasts of each model along with prediction intervals"


autoplot(fc_trends, alpha = 0.8, level = 80) +
  geom_line(
    data = retail_data_updated |> tail(120),
    aes(x = Month, y = Turnover, linetype = "Turnover (in mil$)")
  ) +
  guides(
    color = guide_legend(title = "Forecast model"),
    fill = guide_legend(title = "Forecast model"),
    linetype = guide_legend("ABS data")
  ) +
  labs(
    level = "Level",
    x = "Timeline",
    y = "Turnover (in mil $)",
    title = "Forecast prediction comparison with the ABS released data",
    subtitle = "Latest ABS release: March 2024",
    caption = "Source: Australian Bureau of Statistics"
  )  + 
  theme_minimal() 
```




```{r}
#| label: tbl-forecomprmse
#| tbl-cap: "Comparison of forecasts for each model on the latest ABS data"

forecast_rmse <-
bind_rows(
    fit_new |> forecast(h = 24) |> accuracy(retail_data_updated)
  ) |> filter(.type == "Test") %>%
  select(-ME, -MPE, -ACF1,-State,-Industry,-.type) %>% arrange(RMSE) 

forecast_rmse %>% kable(digits = 2, align = "r") %>%
  kable_styling(full_width = T) %>%
  row_spec(0, color = "white", background = "#FFAF33", bold = TRUE) 
```


:::{.callout-note}
# Comparison of forecasts with ABS released data

@fig-forecastnewplot illustrates the model forecast predictions of the ETS and ARIMA model with the latest ABS released turnover data. Based on this, we can make the following observations:

1. Each of the models were observed to forecast the seasonality and the trend of the data effectively. 
2. The point forecasts by the ETS model were observed to follow the actual ABS released turnover data relatively closer than the SARIMA model.
3. For each of the two models, the ABS released turnover data was well within the 80 % prediction interval and closely followed the point forecasts of the respective models.
4. The ETS model successfully forecasted the peak of retail turnover with better accuracy, while the SARIMA model exhibited a tendency to slightly over-forecast the peak.
5. @tbl-forecomprmse illustrates the point forecast accuracies for the 2 models based on the data available until the end of March 2024. The RMSE values suggest that the ETS model provided forecasts of marginally better accuracy than the SARIMA model.

:::


# Benefits and limitations of the chosen forecast models

The benefits and the limitations of the ETS (MAM) and the SARIMA (1,1,1)[0,1,2] (m=12) are delineated below.

<div style="display: flex; gap: 20px;">


<div style="flex: 1; border: 1px solid #ccc; padding: 10px; border-radius: 5px;background-color: #f0f8ff;">
__Benefits__

1. __Suitability based on model forecasts__ 

   - Based on our forecasts, as illustrated by @fig-forecastnewplot, we can observe that both the ETS and ARIMA models predicted turnovers very close to the actual data. This demonstrates that each model can accurately forecast the seasonality and trend of the retail data.

2. __Working methodology__

   - Due to the inherent structure of the ETS model, the decomposition of the timeseries data into errors, trend and seasonality make it easier and interpretable to choose a model for forecasting the retail data based on the historical data.
   - The ARIMA model on the other hand forecasts the future based on the autocorrelations of the historical data (called lags). While this model is not as straight forward to interpret, however, as the model relies on the changes in the past to forecast the future, this typically allows the model to rapidly adjust for sudden changes in trend or seasonality, resulting in more accurate forecasts. As the model works primarily on the lags of the data, it is additionally suitable to handle cyclic data too. 

3. __Model simplicity__ 

   - As the ETS model can deal with multiplicative seasonality, we are not required to transform our timeseries data to stationary data. This makes the ETS model a simple one to work with.
   - The ARIMA model, however, cannot handle non-stationary data directly. It requires transforming the data through seasonal and regular differencing to achieve stationarity before accurate forecasts can be made. While the model is more complex to interpret and involves several preparatory steps, it is a strong choice for forecasting based on the attributes of the time series data. 


</div>

<div style="flex: 1; border: 1px solid #ccc; padding: 10px; border-radius: 5px;background-color: #ffe4e1;">


__Limitations__

1. __Model sensitivity__

   - The ARIMA model is generally sensitive to the changes in the recent trend and seasonality of the data. Therefore, __a one-time event such as COVID-19 can significantly affect the future forecasts of the data.__ 
   - On the other hand, the __ETS model looks at the entire history of the timeseries and is less sensitive to sudden depressions such as COVID-19.__


2. __Inability to use exogenous variables__

   - Both the ARIMA and the ETS models __cannot accommodate the effect exogenous variables.__ 
   - Exogenous variables can be handled by models such as dynamic regression through dummy variables.
</div>

</div>

